import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, ndcg_score
from torch_geometric.data import Data
from torch_geometric.utils import from_scipy_sparse_matrix
from scipy.sparse import csr_matrix
from model import SparseGAT
import torch.nn.functional as F


def precision_at_k(y_true, y_scores, k):
    """
    è®¡ç®—Precision@k
    :param y_true: çœŸå®æ ‡ç­¾ (n_samples,)
    :param y_scores: é¢„æµ‹æ¦‚ç‡åˆ†æ•° (n_samples,)
    :param k: å‰kä¸ªé¢„æµ‹
    :return: Precision@k
    """
    # è·å–å‰kä¸ªæœ€é«˜åˆ†æ•°çš„ç´¢å¼•
    top_k_indices = np.argsort(y_scores)[::-1][:k]
    # è®¡ç®—å‰kä¸ªä¸­çœŸå®æ­£æ ·æœ¬çš„æ•°é‡
    relevant_in_top_k = np.sum(y_true[top_k_indices])
    return relevant_in_top_k / k


def recall_at_k(y_true, y_scores, k):
    """
    è®¡ç®—Recall@k
    :param y_true: çœŸå®æ ‡ç­¾ (n_samples,)
    :param y_scores: é¢„æµ‹æ¦‚ç‡åˆ†æ•° (n_samples,)
    :param k: å‰kä¸ªé¢„æµ‹
    :return: Recall@k
    """
    # è·å–å‰kä¸ªæœ€é«˜åˆ†æ•°çš„ç´¢å¼•
    top_k_indices = np.argsort(y_scores)[::-1][:k]
    # è®¡ç®—å‰kä¸ªä¸­çœŸå®æ­£æ ·æœ¬çš„æ•°é‡
    relevant_in_top_k = np.sum(y_true[top_k_indices])
    # è®¡ç®—æ€»çš„çœŸå®æ­£æ ·æœ¬æ•°é‡
    total_relevant = np.sum(y_true)
    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0


def f1_at_k(y_true, y_scores, k):
    """
    è®¡ç®—F1@k
    :param y_true: çœŸå®æ ‡ç­¾ (n_samples,)
    :param y_scores: é¢„æµ‹æ¦‚ç‡åˆ†æ•° (n_samples,)
    :param k: å‰kä¸ªé¢„æµ‹
    :return: F1@k
    """
    p_at_k = precision_at_k(y_true, y_scores, k)
    r_at_k = recall_at_k(y_true, y_scores, k)
    if p_at_k + r_at_k == 0:
        return 0.0
    return 2 * p_at_k * r_at_k / (p_at_k + r_at_k)


def ndcg_at_k(y_true, y_scores, k):
    """
    è®¡ç®—NDCG@k
    :param y_true: çœŸå®æ ‡ç­¾ (n_samples,)
    :param y_scores: é¢„æµ‹æ¦‚ç‡åˆ†æ•° (n_samples,)
    :param k: å‰kä¸ªé¢„æµ‹
    :return: NDCG@k
    """
    # å°†y_trueå’Œy_scoresè½¬æ¢ä¸º2Dæ•°ç»„æ ¼å¼ï¼Œç¬¦åˆsklearnçš„ndcg_scoreè¦æ±‚
    y_true_2d = y_true.reshape(1, -1)
    y_scores_2d = y_scores.reshape(1, -1)
    
    try:
        return ndcg_score(y_true_2d, y_scores_2d, k=k)
    except:
        # å¦‚æœsklearnçš„ndcg_scoreå¤±è´¥ï¼Œä½¿ç”¨è‡ªå®šä¹‰å®ç°
        return custom_ndcg_at_k(y_true, y_scores, k)


def custom_ndcg_at_k(y_true, y_scores, k):
    """
    è‡ªå®šä¹‰NDCG@kå®ç°
    """
    # è·å–å‰kä¸ªæœ€é«˜åˆ†æ•°çš„ç´¢å¼•
    top_k_indices = np.argsort(y_scores)[::-1][:k]
    
    # è®¡ç®—DCG
    dcg = 0.0
    for i, idx in enumerate(top_k_indices):
        dcg += y_true[idx] / np.log2(i + 2)  # log2(i+2) å› ä¸ºiä»0å¼€å§‹
    
    # è®¡ç®—IDCG (ç†æƒ³æƒ…å†µä¸‹çš„DCG)
    ideal_scores = np.sort(y_true)[::-1][:k]
    idcg = 0.0
    for i, score in enumerate(ideal_scores):
        idcg += score / np.log2(i + 2)
    
    return dcg / idcg if idcg > 0 else 0.0


def test_advanced_metrics(features_by_cluster=None, adjs_by_cluster=None, labels_by_cluster=None, 
                         cluster_data=None, model_path="./model/best_model_multi_cluster.pth", k_values=[5, 10, 20]):
    """
    ä½¿ç”¨é«˜çº§æŒ‡æ ‡æµ‹è¯•å¤šç±»åˆ«å¸æœºæ¨¡å‹
    
    :param features_by_cluster: æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›cluster_dataåˆ™å¿½ç•¥ï¼‰
    :param adjs_by_cluster: æ¯ä¸ªç±»åˆ«çš„é‚»æ¥çŸ©é˜µå­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›cluster_dataåˆ™å¿½ç•¥ï¼‰
    :param labels_by_cluster: æ¯ä¸ªç±»åˆ«çš„æ ‡ç­¾å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›cluster_dataåˆ™å¿½ç•¥ï¼‰
    :param cluster_data: è®­ç»ƒæ—¶è¿”å›çš„cluster_dataï¼ŒåŒ…å«é¢„å¤„ç†å¥½çš„æ•°æ®å’Œåˆ’åˆ†ä¿¡æ¯
    :param model_path: æ¨¡å‹è·¯å¾„
    :param k_values: è¦æµ‹è¯•çš„kå€¼åˆ—è¡¨
    """
    hidden_dim = 16
    dropout = 0.2
    alpha = 0.2
    nheads = 8
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # å¦‚æœæä¾›äº†cluster_dataï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™é‡æ–°æ„å»ºæ•°æ®
    if cluster_data is not None:
        print("Using pre-built cluster data from training...")
        test_cluster_data = cluster_data
        # è·å–ç‰¹å¾ç»´åº¦
        if cluster_data:
            first_cluster_id = next(iter(cluster_data.keys()))
            total_features_dim = cluster_data[first_cluster_id]['data'].x.shape[1]
            print(f"Feature dimension: {total_features_dim}")
        else:
            print("Empty cluster_data provided")
            return None, None
    else:
        print("Building cluster data from scratch...")
        if features_by_cluster is None or adjs_by_cluster is None or labels_by_cluster is None:
            print("Error: features_by_cluster, adjs_by_cluster, and labels_by_cluster must be provided when cluster_data is None")
            return None, None
        # åŸæœ‰çš„æ•°æ®æ„å»ºé€»è¾‘
        test_cluster_data = {}
        total_features_dim = None
        
        for cluster_id in features_by_cluster.keys():
            if cluster_id == 13:  # è·³è¿‡ç±»åˆ«13
                continue
                
            features = features_by_cluster[cluster_id]
            adjs = adjs_by_cluster[cluster_id]
            labels = labels_by_cluster[cluster_id]
            
            N = len(features)
            
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§
            if total_features_dim is None:
                total_features_dim = len(features[0]) if features else 0
            else:
                current_dim = len(features[0]) if features else 0
                if current_dim != total_features_dim:
                    # å¯¹æ¯ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾è¿›è¡Œå¡«å……æˆ–æˆªæ–­
                    for i in range(len(features)):
                        if len(features[i]) < total_features_dim:
                            features[i].extend([0] * (total_features_dim - len(features[i])))
                        elif len(features[i]) > total_features_dim:
                            features[i] = features[i][:total_features_dim]
            
            # å°†å­—å…¸æ ¼å¼çš„é‚»æ¥è¡¨è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ
            if isinstance(adjs, dict):
                adj_matrix = np.zeros((N, N), dtype=np.float32)
                for u, neighbors in adjs.items():
                    for v, weight in neighbors.items():
                        if u < N and v < N:
                            adj_matrix[u, v] = weight
                adj = csr_matrix(adj_matrix)
            else:
                adj = csr_matrix(adjs)
                
            features_tensor = torch.tensor(features, dtype=torch.float)
            labels_tensor = torch.tensor(labels, dtype=torch.float)
            
            # åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
            labels_array = np.array(labels)
            train_idx, test_idx = train_test_split(np.arange(N), test_size=0.2, stratify=labels_array)
            train_idx, val_idx = train_test_split(train_idx, test_size=0.25, stratify=labels_array[train_idx])
            
            test_mask = torch.zeros(N, dtype=torch.bool)
            test_mask[test_idx] = True
            
            edge_index, edge_weight = from_scipy_sparse_matrix(adj)
            
            data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor, test_mask=test_mask).to(device)
            
            test_cluster_data[cluster_id] = {
                'data': data,
                'test_idx': test_idx
            }
    
    # æ‰“å°æ¯ä¸ªclusterçš„æµ‹è¯•æ ·æœ¬ä¿¡æ¯
    for cluster_id, cluster_info in test_cluster_data.items():
        data = cluster_info['data']
        test_samples = data.test_mask.sum().item()
        positive_samples = data.y[data.test_mask].sum().item()
        print(f'Cluster {cluster_id}: {data.x.shape[0]} nodes, test samples: {test_samples}, positive: {positive_samples}')

    # åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½å‚æ•°
    model = SparseGAT(
        nfeat=total_features_dim,
        nhid=hidden_dim,
        nclass=1,
        dropout=dropout,
        alpha=alpha,
        nheads=nheads
    ).to(device)
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Loaded model from {model_path}")
    except:
        print(f"Failed to load model from {model_path}")
        return None, None
    
    model.eval()

    print("\n=== Advanced Metrics Testing Results ===")
    
    # å­˜å‚¨æ‰€æœ‰ç±»åˆ«çš„ç»“æœ
    all_results = {}
    
    with torch.no_grad():
        for cluster_id, cluster_info in test_cluster_data.items():
            data = cluster_info['data']
            out = model(data.x, data.edge_index)
            scores = torch.sigmoid(out).cpu().numpy()
            
            # è·å–æµ‹è¯•é›†çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°
            test_mask = data.test_mask.cpu().numpy()
            y_true = data.y.cpu().numpy()[test_mask]
            y_scores = scores[test_mask]
            
            print(f"\n--- Cluster {cluster_id} ---")
            print(f"Test samples: {len(y_true)}, Positive samples: {np.sum(y_true)}")
            
            # è®¡ç®—AUC-ROC
            try:
                auc_roc = roc_auc_score(y_true, y_scores)
                print(f"AUC-ROC: {auc_roc:.4f}")
            except:
                print("AUC-ROC: N/A (insufficient positive samples)")
                auc_roc = 0.0
            
            cluster_results = {'auc_roc': auc_roc}
            
            # è®¡ç®—ä¸åŒkå€¼çš„æŒ‡æ ‡
            for k in k_values:
                if k <= len(y_true):
                    p_at_k = precision_at_k(y_true, y_scores, k)
                    r_at_k = recall_at_k(y_true, y_scores, k)
                    f1_at_k_val = f1_at_k(y_true, y_scores, k)
                    ndcg_at_k_val = ndcg_at_k(y_true, y_scores, k)
                    
                    print(f"k={k}: P@{k}={p_at_k:.4f}, R@{k}={r_at_k:.4f}, F1@{k}={f1_at_k_val:.4f}, NDCG@{k}={ndcg_at_k_val:.4f}")
                    
                    cluster_results[f'p_at_{k}'] = p_at_k
                    cluster_results[f'r_at_{k}'] = r_at_k
                    cluster_results[f'f1_at_{k}'] = f1_at_k_val
                    cluster_results[f'ndcg_at_{k}'] = ndcg_at_k_val
                else:
                    print(f"k={k}: N/A (k > number of test samples)")
                    cluster_results[f'p_at_{k}'] = 0.0
                    cluster_results[f'r_at_{k}'] = 0.0
                    cluster_results[f'f1_at_{k}'] = 0.0
                    cluster_results[f'ndcg_at_{k}'] = 0.0
            
            all_results[cluster_id] = cluster_results
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    print("\n=== Average Metrics Across All Clusters ===")
    avg_metrics = {}
    
    # è®¡ç®—AUC-ROCå¹³å‡å€¼
    auc_rocs = [results['auc_roc'] for results in all_results.values()]
    avg_auc_roc = np.mean(auc_rocs)
    print(f"Average AUC-ROC: {avg_auc_roc:.4f}")
    avg_metrics['avg_auc_roc'] = avg_auc_roc
    
    # è®¡ç®—ä¸åŒkå€¼çš„å¹³å‡æŒ‡æ ‡
    for k in k_values:
        p_at_k_vals = [results[f'p_at_{k}'] for results in all_results.values()]
        r_at_k_vals = [results[f'r_at_{k}'] for results in all_results.values()]
        f1_at_k_vals = [results[f'f1_at_{k}'] for results in all_results.values()]
        ndcg_at_k_vals = [results[f'ndcg_at_{k}'] for results in all_results.values()]
        
        avg_p_at_k = np.mean(p_at_k_vals)
        avg_r_at_k = np.mean(r_at_k_vals)
        avg_f1_at_k = np.mean(f1_at_k_vals)
        avg_ndcg_at_k = np.mean(ndcg_at_k_vals)
        
        print(f"Average k={k}: P@{k}={avg_p_at_k:.4f}, R@{k}={avg_r_at_k:.4f}, "
              f"F1@{k}={avg_f1_at_k:.4f}, NDCG@{k}={avg_ndcg_at_k:.4f}")
        
        avg_metrics[f'avg_p_at_{k}'] = avg_p_at_k
        avg_metrics[f'avg_r_at_{k}'] = avg_r_at_k
        avg_metrics[f'avg_f1_at_{k}'] = avg_f1_at_k
        avg_metrics[f'avg_ndcg_at_{k}'] = avg_ndcg_at_k
    
    return all_results, avg_metrics


if __name__ == "__main__":
    # ä»…æµ‹è¯•æµç¨‹ï¼ŒåŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹å’Œæ•°æ®åˆ’åˆ†
    from data_processing import data_prepare
    from train import test_multi_cluster
    
    print("=== Starting Testing Pipeline (No Training) ===")
    
    # æ•°æ®è·¯å¾„é…ç½®
    poi_path = 'E:/Project/traffic/order_data/examine_neighbor_poi_anomaly_detect_chengdu_in_500_1101.txt'
    driver_order_path = 'E:/Project/traffic/order_data/order_driver_01.txt'
    ground_truth_path = 'E:/Project/traffic/order_data/ground_truth.xlsx'
    grid_granularity = 500
    
    # æ•°æ®å‡†å¤‡
    print("=== Data Preparation ===")
    features_by_cluster, adjs_by_cluster, labels_by_cluster, driver_clusters, cluster_boundaries = data_prepare(
        poi_path, driver_order_path, ground_truth_path, grid_granularity
    )
    
    # é‡å»ºè®­ç»ƒæ—¶çš„æ•°æ®ç»“æ„ï¼ˆä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ’åˆ†é€»è¾‘ï¼‰
    print("=== Rebuilding Training Data Structure ===")
    from sklearn.model_selection import train_test_split
    from torch_geometric.data import Data
    from torch_geometric.utils import from_scipy_sparse_matrix
    from scipy.sparse import csr_matrix
    import torch
    import numpy as np
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    cluster_data = {}
    total_features_dim = 782  # æ ¹æ®è®­ç»ƒæ—¶çš„ç‰¹å¾ç»´åº¦
    
    for cluster_id in features_by_cluster.keys():
        if cluster_id == 11:
            continue
        features = features_by_cluster[cluster_id]
        adjs = adjs_by_cluster[cluster_id]
        labels = labels_by_cluster[cluster_id]
        
        N = len(features)
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§
        for i in range(len(features)):
            if len(features[i]) < total_features_dim:
                features[i].extend([0] * (total_features_dim - len(features[i])))
            elif len(features[i]) > total_features_dim:
                features[i] = features[i][:total_features_dim]
        
        # å°†å­—å…¸æ ¼å¼çš„é‚»æ¥è¡¨è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ
        if isinstance(adjs, dict):
            adj_matrix = np.zeros((N, N), dtype=np.float32)
            for u, neighbors in adjs.items():
                for v, weight in neighbors.items():
                    if u < N and v < N:
                        adj_matrix[u, v] = weight
            adj = csr_matrix(adj_matrix)
        else:
            adj = csr_matrix(adjs)
            
        features_tensor = torch.tensor(features, dtype=torch.float)
        labels_tensor = torch.tensor(labels, dtype=torch.float)
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®åˆ’åˆ†ï¼ˆå›ºå®šéšæœºç§å­ç¡®ä¿ä¸€è‡´æ€§ï¼‰
        np.random.seed(42)  # ç¡®ä¿æ•°æ®åˆ’åˆ†ä¸€è‡´
        labels_array = np.array(labels)
        train_idx, test_idx = train_test_split(np.arange(N), test_size=0.2, stratify=labels_array, random_state=42)
        train_idx, val_idx = train_test_split(train_idx, test_size=0.25, stratify=labels_array[train_idx], random_state=42)
        
        train_mask = torch.zeros(N, dtype=torch.bool)
        val_mask = torch.zeros(N, dtype=torch.bool)
        test_mask = torch.zeros(N, dtype=torch.bool)
        train_mask[train_idx] = True
        val_mask[val_idx] = True
        test_mask[test_idx] = True
        
        edge_index, edge_weight = from_scipy_sparse_matrix(adj)
        
        data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor,
                   train_mask=train_mask, val_mask=val_mask, test_mask=test_mask).to(device)
        
        cluster_data[cluster_id] = {
            'data': data,
            'train_idx': train_idx,
            'val_idx': val_idx,
            'test_idx': test_idx
        }
        
        print(f'Cluster {cluster_id}: {N} nodes, test samples: {len(test_idx)}, positive: {labels_tensor[test_idx].sum().item()}')
    
    # æµ‹è¯•å¤šç±»åˆ«æ¨¡å‹ï¼ˆä¼ ç»ŸæŒ‡æ ‡ï¼‰
    print("\n=== Multi-Cluster Testing (Traditional Metrics) ===")
    test_multi_cluster(features_by_cluster, adjs_by_cluster, labels_by_cluster)
    
    # æµ‹è¯•å¤šç±»åˆ«æ¨¡å‹ï¼ˆé«˜çº§æŒ‡æ ‡ï¼‰- ä½¿ç”¨é‡å»ºçš„æ•°æ®ç»“æ„
    print("\n=== Multi-Cluster Testing (Advanced Metrics) ===")
    try:
        results, avg_metrics = test_advanced_metrics(cluster_data=cluster_data)
        
        if results is not None and avg_metrics is not None:
            # è¾“å‡ºæœ€ç»ˆç»“æœæ‘˜è¦
            print("\n=== Final Results Summary ===")
            print(f"Average AUC-ROC: {avg_metrics['avg_auc_roc']:.4f}")
            for k in [5, 10, 20]:
                print(f"Average F1@{k}: {avg_metrics[f'avg_f1_at_{k}']:.4f}")
                print(f"Average NDCG@{k}: {avg_metrics[f'avg_ndcg_at_{k}']:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡F1åˆ†æ•°
            target_f1 = 0.6
            best_f1_at_k = max([avg_metrics[f'avg_f1_at_{k}'] for k in [5, 10, 20]])
            
            print(f"\n=== Performance Analysis ===")
            print(f"Best F1@k score: {best_f1_at_k:.4f}")
            print(f"Target F1 score: {target_f1:.4f}")
            
            if best_f1_at_k >= target_f1:
                print("ğŸ‰ SUCCESS: Target F1 score achieved!")
            else:
                print(f"âŒ Target not reached. Need improvement of {target_f1 - best_f1_at_k:.4f}")
                print("\nSuggested optimizations for train.py:")
                print("1. Increase learning rate from 5e-4 to 1e-3")
                print("2. Increase hidden dimensions from 16 to 32 or 64")
                print("3. Increase attention heads from 8 to 12 or 16")
                print("4. Adjust loss function weights (try 0.3 focal + 0.7 f1)")
                print("5. Reduce dropout from 0.2 to 0.1")
                print("6. Increase training epochs if early stopping")
        else:
            print("Advanced metrics testing returned None")
    except Exception as e:
        print(f"Advanced metrics testing failed: {e}")
        import traceback
        traceback.print_exc() 